{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectiorizer Code\n",
    "1. Create the model\n",
    "model = WhateverModelYouLike()\n",
    "\n",
    "2. Train / Fit the model\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "3. Check the model's performance\n",
    "model.score(X_train, Y_train)\n",
    "model.score(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/carlosm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/carlosm/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/carlosm/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/shivamkushwaha/bbc-full-text-document-classification\n",
    "!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating dataset\n",
    "\n",
    "df = pd.read_csv('bbc_text_cls.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Ad sales boost Time Warner profit\\n\\nQuarterly...\n",
      "1       Dollar gains on Greenspan speech\\n\\nThe dollar...\n",
      "2       Yukos unit buyer faces loan claim\\n\\nThe owner...\n",
      "3       High fuel prices hit BA's profits\\n\\nBritish A...\n",
      "4       Pernod takeover talk lifts Domecq\\n\\nShares in...\n",
      "                              ...                        \n",
      "2220    BT program to beat dialler scams\\n\\nBT is intr...\n",
      "2221    Spam e-mails tempt net shoppers\\n\\nComputer us...\n",
      "2222    Be careful how you code\\n\\nA new European dire...\n",
      "2223    US cyber security chief resigns\\n\\nThe man mak...\n",
      "2224    Losing yourself in online gaming\\n\\nOnline rol...\n",
      "Name: text, Length: 2225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Separating columns (inputs, targets)\n",
    "\n",
    "inputs = df['text']\n",
    "labels = df['labels']\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the label's values frequency\n",
    "\n",
    "labels.hist(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting training / testings datasets\n",
    "\n",
    "inputs_train, inputs_test, Y_train, Y_test = train_test_split(\n",
    "    inputs, labels, random_state=123\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vectorizer Object\n",
    "\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X as vectors\n",
    "\n",
    "X_train = vectorizer.fit_transform(inputs_train) # \"fit\" learns the vocabulary for training\n",
    "X_test = vectorizer.transform(inputs_test) # without \"fit\" uses the vocabulary learned in training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337411"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007695239935415004"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What percentage of values are non-zero (sparse metric)\n",
    "\n",
    "(X_train != 0).sum() / (np.prod(X_train.shape))\n",
    "\n",
    "# If close to zero Sparse representation is OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9802158273381295\n",
      "Test score:  0.933572710951526\n"
     ]
    }
   ],
   "source": [
    "# Modeling and testing\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train score: \", model.score(X_train, Y_train))\n",
    "print(\"Test score: \", model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing with nlp Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9892086330935251\n",
      "Test score:  0.9605026929982047\n"
     ]
    }
   ],
   "source": [
    "# Filtering stop words\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(inputs_train)\n",
    "X_test = vectorizer.transform(inputs_test)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train score: \", model.score(X_train, Y_train))\n",
    "print(\"Test score: \", model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        words_and_tags = nltk.pos_tag(tokens)\n",
    "        return list(self.wnl.lemmatize(word, pos=get_wordnet_pos(tag)) \n",
    "            for word, tag in words_and_tags\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlosm/Development/python/courses/nlp_udemy/venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/carlosm/Development/python/courses/nlp_udemy/venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9886091127098321\n",
      "Test score:  0.9461400359066428\n"
     ]
    }
   ],
   "source": [
    "# Using lemmatization\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words='english')\n",
    "X_train = vectorizer.fit_transform(inputs_train)\n",
    "X_test = vectorizer.transform(inputs_test)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train score: \", model.score(X_train, Y_train))\n",
    "print(\"Test score: \", model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemTokenizer:\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        return [self.porter.stem(t) for t in tokens]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlosm/Development/python/courses/nlp_udemy/venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/carlosm/Development/python/courses/nlp_udemy/venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9856115107913669\n",
      "Test score:  0.9461400359066428\n"
     ]
    }
   ],
   "source": [
    "# Using Stemming\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=StemTokenizer(), stop_words='english')\n",
    "X_train = vectorizer.fit_transform(inputs_train)\n",
    "X_test = vectorizer.transform(inputs_test)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train score: \", model.score(X_train, Y_train))\n",
    "print(\"Test score: \", model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Tokenizer (Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(s):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlosm/Development/python/courses/nlp_udemy/venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9766187050359713\n",
      "Test score:  0.9120287253141831\n"
     ]
    }
   ],
   "source": [
    "# Using simple tokenizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=simple_tokenizer)\n",
    "X_train = vectorizer.fit_transform(inputs_train)\n",
    "X_test = vectorizer.transform(inputs_test)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train score: \", model.score(X_train, Y_train))\n",
    "print(\"Test score: \", model.score(X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
